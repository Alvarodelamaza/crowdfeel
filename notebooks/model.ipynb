{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "812f6bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9233c4a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/lodeizen/.pyenv/versions/3.8.12/envs/crowdfeel/lib/python3.8/site-packages (4.21.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/lodeizen/.pyenv/versions/3.8.12/envs/crowdfeel/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lodeizen/.pyenv/versions/3.8.12/envs/crowdfeel/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/lodeizen/.pyenv/versions/3.8.12/envs/crowdfeel/lib/python3.8/site-packages (from transformers) (2022.8.17)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/lodeizen/.pyenv/versions/3.8.12/envs/crowdfeel/lib/python3.8/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: requests in /Users/lodeizen/.pyenv/versions/3.8.12/envs/crowdfeel/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: filelock in /Users/lodeizen/.pyenv/versions/3.8.12/envs/crowdfeel/lib/python3.8/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/lodeizen/.pyenv/versions/3.8.12/envs/crowdfeel/lib/python3.8/site-packages (from transformers) (0.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/lodeizen/.pyenv/versions/3.8.12/envs/crowdfeel/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/lodeizen/.pyenv/versions/3.8.12/envs/crowdfeel/lib/python3.8/site-packages (from transformers) (1.23.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/lodeizen/.pyenv/versions/3.8.12/envs/crowdfeel/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/lodeizen/.pyenv/versions/3.8.12/envs/crowdfeel/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/lodeizen/.pyenv/versions/3.8.12/envs/crowdfeel/lib/python3.8/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lodeizen/.pyenv/versions/3.8.12/envs/crowdfeel/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/lodeizen/.pyenv/versions/3.8.12/envs/crowdfeel/lib/python3.8/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lodeizen/.pyenv/versions/3.8.12/envs/crowdfeel/lib/python3.8/site-packages (from requests->transformers) (2022.6.15)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-gpu (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-gpu\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install tensorflow-gpu\n",
    "from transformers import BertTokenizer, TFBertModel, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f65bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "# For sending GET requests from the API\n",
    "import requests\n",
    "# For saving access tokens and for file management when creating and adding to the dataset\n",
    "import os\n",
    "# For dealing with json responses we receive from the API\n",
    "import json\n",
    "# For displaying the data after\n",
    "import pandas as pd\n",
    "# For saving the response data in CSV format\n",
    "import csv\n",
    "# For parsing the dates received from twitter in readable formats\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "import unicodedata\n",
    "#To add wait time between requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eb40942b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY _TheSpecialOne_  \\\n",
       "0  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   scotthamilton   \n",
       "1  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY        mattycus   \n",
       "2  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         ElleCTF   \n",
       "3  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          Karoli   \n",
       "4  0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY        joy_wolf   \n",
       "\n",
       "  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0  is upset that he can't update his Facebook by ...                                                                   \n",
       "1  @Kenichan I dived many times for the ball. Man...                                                                   \n",
       "2    my whole body feels itchy and like its on fire                                                                    \n",
       "3  @nationwideclass no, it's not behaving at all....                                                                   \n",
       "4                      @Kwesidei not the whole crew                                                                    "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('../raw_data/training_data.csv',   encoding='latin-1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8498730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns=['label','id', 'date','query','username','tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3768d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['label'])\n",
    "y = data[['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "302adc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599999 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                          date     query         username  \\\n",
       "0        1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "1        1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "2        1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "3        1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "4        1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY         joy_wolf   \n",
       "...             ...                           ...       ...              ...   \n",
       "1599994  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY  AmandaMarie1028   \n",
       "1599995  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY      TheWDBoards   \n",
       "1599996  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY           bpbabe   \n",
       "1599997  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY     tinydiamondz   \n",
       "1599998  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   RyanTrevMorris   \n",
       "\n",
       "                                                     tweet  \n",
       "0        is upset that he can't update his Facebook by ...  \n",
       "1        @Kenichan I dived many times for the ball. Man...  \n",
       "2          my whole body feels itchy and like its on fire   \n",
       "3        @nationwideclass no, it's not behaving at all....  \n",
       "4                            @Kwesidei not the whole crew   \n",
       "...                                                    ...  \n",
       "1599994  Just woke up. Having no school is the best fee...  \n",
       "1599995  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599996  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599997  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599998  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1599999 rows x 5 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "895e3516",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = 800000/159999\n",
    "tweet = data.drop(columns=['query','label','id','date','username'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2001834d",
   "metadata": {},
   "source": [
    "\n",
    "for tweet in range(0, len(X)):  \n",
    "    # Remove all the special characters\n",
    "    processed_tweet = re.sub(r'\\W', ' ', str(X[tweet]))\n",
    " \n",
    "    # remove all single characters\n",
    "    processed_tweet = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_tweet)\n",
    " \n",
    "    # Remove single characters from the start\n",
    "    processed_tweet = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_tweet) \n",
    " \n",
    "    # Substituting multiple spaces with single space\n",
    "    processed_tweet= re.sub(r'\\s+', ' ', processed_tweet, flags=re.I)\n",
    " \n",
    "    # Removing prefixed 'b'\n",
    "    processed_tweet = re.sub(r'^b\\s+', '', processed_tweet)\n",
    " \n",
    "    # Converting to Lowercase\n",
    "    processed_tweet = processed_tweet.lower()\n",
    " \n",
    "    processed_tweets.append(processed_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1041f6e4",
   "metadata": {},
   "source": [
    "# raw data folder: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce0d405",
   "metadata": {},
   "source": [
    "data_16k = data.sample(frac=0.01).reset_index(drop=True)\n",
    "X_train_16k, X_test_16k, y_train_16k, y_test_16k = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d308179c",
   "metadata": {},
   "source": [
    "data_1600k = data.sample(frac=0.5).reset_index(drop=True)\n",
    "X_train_1600k, X_test_1600k, y_train_1600k, y_test_1600k = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936f3142",
   "metadata": {},
   "source": [
    "csvs_names = ['X_train_16k', 'X_test_16k', 'y_train_16k', 'y_test_16k', 'X_train_1600k', 'X_test_1600k', 'y_train_1600k', 'y_test_1600k']\n",
    "csvs = [X_train_16k, X_test_16k, y_train_16k, y_test_16k, X_train_1600k, X_test_1600k, y_train_1600k, y_test_1600k]\n",
    "\n",
    "for i in csvs:\n",
    "    for f in csvs_names:\n",
    "        i.to_csv(f'/Users/home/code/tjebbel/crowdfeel/raw_data/{f}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f380246",
   "metadata": {},
   "source": [
    "Twitter API toolsQuery Builder\n",
    "Query Builder\n",
    "Easily craft working queries or filtered stream rules for the v2 search and streaming endpoints.\n",
    "\n",
    "How to use this tool? \n",
    "\n",
    "Query search v2 language is used for filtering tweets in the Twitter V2 API.\n",
    "\n",
    "Endpoints using query search: GET /2/tweets/search/recent, GET /2/tweets/search/all, POST /2/tweets/search/stream/rules\n",
    "\n",
    "For creating a new query, add filters and groups or you can also type filters in the textarea, the UI will automatically update.\n",
    "\n",
    "For editing existing query, simply paste your query in the textarea and make your changes.\n",
    "Query search v2\n",
    "Explore more Twitter API tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "50e3836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_mask, y_train, y_mask =\\\n",
    "    train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "921c412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unmasker = pipeline('fill-mask', model='bert-base-multilingual-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4addd5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /Users/lodeizen/.pyenv/versions/3.8.12/envs/crowdfeel/lib/python3.8/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /Users/lodeizen/.pyenv/versions/3.8.12/envs/crowdfeel/lib/python3.8/site-packages (from nltk) (4.64.0)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/lodeizen/.pyenv/versions/3.8.12/envs/crowdfeel/lib/python3.8/site-packages (from nltk) (2022.8.17)\n",
      "Installing collected packages: click, nltk\n",
      "Successfully installed click-8.1.3 nltk-3.7\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7648f23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lodeizen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk  \n",
    "nltk.download('stopwords')  \n",
    "from nltk.corpus import stopwords  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "65193ca2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max_df corresponds to < documents than min_df",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [73]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer  \n\u001b[1;32m      2\u001b[0m tfidfconverter \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, stop_words\u001b[38;5;241m=\u001b[39mstopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))  \n\u001b[0;32m----> 3\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mtfidfconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/crowdfeel/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:2079\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2072\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2074\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2075\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2076\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2077\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2078\u001b[0m )\n\u001b[0;32m-> 2079\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2080\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2081\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/crowdfeel/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1352\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1348\u001b[0m min_doc_count \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1349\u001b[0m     min_df \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(min_df, numbers\u001b[38;5;241m.\u001b[39mIntegral) \u001b[38;5;28;01melse\u001b[39;00m min_df \u001b[38;5;241m*\u001b[39m n_doc\n\u001b[1;32m   1350\u001b[0m )\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_doc_count \u001b[38;5;241m<\u001b[39m min_doc_count:\n\u001b[0;32m-> 1352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_df corresponds to < documents than min_df\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1354\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sort_features(X, vocabulary)\n",
      "\u001b[0;31mValueError\u001b[0m: max_df corresponds to < documents than min_df"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c151cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "text_classifier = RandomForestClassifier(n_estimators=100, random_state=0)  \n",
    "text_classifier.fit(X_train, y_train)\n",
    "\n",
    "predictions = text_classifier.predict(X_test)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48f9821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    " \n",
    "print(confusion_matrix(y_test,predictions))  \n",
    "print(classification_report(y_test,predictions))  \n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feadb74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5f6c1e8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nTFBertModel requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://www.tensorflow.org/install and follow the ones that match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-multilingual-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTFBertModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-multilingual-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m, do_lower_case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/crowdfeel/lib/python3.8/site-packages/transformers/utils/import_utils.py:918\u001b[0m, in \u001b[0;36mDummyObject.__getattr__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, key)\n\u001b[0;32m--> 918\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/crowdfeel/lib/python3.8/site-packages/transformers/utils/import_utils.py:906\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m    904\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m--> 906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nTFBertModel requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://www.tensorflow.org/install and follow the ones that match your environment.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "model = TFBertModel.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "model = model.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "#tokenizer_f = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ebc9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETTING MODEL\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6, output_attentions=False, output_hidden_states=False )\n",
    "model_u = model.from_pretrained('bert-base-uncased')\n",
    "tokenizer_u = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "#tokenizer_f = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#CREATING DATA LOADERS\n",
    "#dataloader_train = DataLoader(dataset_train,sampler = RandomSampler(dataset_train), batch_size= 32)\n",
    "                             \n",
    "#dataloader_val = DataLoader(dataset_val, sampler = RandomSampler(dataset_val), batch_size= 32)\n",
    "      \n",
    "#SETTING OPTIMIZERS\n",
    "#op = AdamW(model.parameters(),lr=1e-5,eps=1e-8)\n",
    "#epochs = 10\n",
    "#scheduler = get_linear_schedule_with_warmup(op, num_warmup_steps=0, num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f7c4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ca6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION TO CALCULATE F1 SCORE\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds,axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average = 'weighted')\n",
    "#FUNCTION FOR CALCULATING ACCURACY PER CLASS\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v:k for k,v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds,axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(label_dict_inverse[label])\n",
    "        print(\"accuracy \", len(y_preds[y_preds==label])/len(y_true))\n",
    "#FUNCTION FOR MODEL EVALUATION\n",
    "def evaluate(dataloader_val):\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37717bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa18d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader_train, \n",
    "                        desc ='Epoch {:1d}'.format(epoch),\n",
    "                        leave=False,\n",
    "                       disable=False\n",
    "                       )\n",
    "    for batch in progress_bar:\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = { 'input_ids' : batch[0],\n",
    "                 'attention_mask' : batch[1],\n",
    "                 'labels' : batch[2]\n",
    "                 }\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss':'{:.3f}'.format(loss.item()/len(batch))})\n",
    "    \n",
    "    # THIS SECTION OF CODE IS JUST FOR PRINTING VALUES AFTER EACH EPOCH.\n",
    "    torch.save(model.state_dict(), f'BERT_ft_epoch{epoch}.model')\n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)\n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_val)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 score (weighted): {val_f1}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0196927e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7b20a7a",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8e1168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download & load GPT-2 model\n",
    "gpt2_generator = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25419fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 3 different sentences\n",
    "# results are sampled from the top 50 candidates\n",
    "sentences = gpt2_generator(\"To be honest, neural networks\", do_sample=True, top_k=50, temperature=0.6, max_length=128, num_return_sequences=3)\n",
    "for sentence in sentences:\n",
    "  print(sentence[\"generated_text\"])\n",
    "  print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a59fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# download & load GPT-J model! It's 22.5GB in size\n",
    "gpt_j_generator = pipeline('text-generation', model='EleutherAI/gpt-j-6B')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d626144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# generate sentences with TOP-K sampling\n",
    "sentences = gpt_j_generator(\"To be honest, robots will\", do_sample=True, top_k=50, temperature=0.6, max_length=128, num_return_sequences=3)\n",
    "for sentence in sentences:\n",
    "  print(sentence[\"generated_text\"])\n",
    "  print(\"=\"*50)\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565769e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.Series([get_auc_CV(MultinomialNB(i))\n",
    "                 for i in np.arange(1, 10, 0.1)],\n",
    "                index=np.arange(1, 10, 0.1))\n",
    "\n",
    "best_alpha = np.round(res.idxmax(), 2)\n",
    "print('Best alpha: ', best_alpha)\n",
    "\n",
    "plt.plot(res)\n",
    "plt.title('AUC vs. Alpha')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('AUC')\n",
    "plt.show()\n",
    "\n",
    "# Compute predicted probabilities\n",
    "nb_model = MultinomialNB(alpha=1.8)\n",
    "nb_model.fit(X_train, y_train)\n",
    "probs = nb_model.predict_proba(X_mask)\n",
    "\n",
    "# Evaluate the classifier\n",
    "evaluate_roc(probs, y_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09d87b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.Series([get_auc_CV(MultinomialNB(i))\n",
    "                 for i in np.arange(1, 10, 0.1)],\n",
    "                index=np.arange(1, 10, 0.1))\n",
    "\n",
    "best_alpha = np.round(res.idxmax(), 2)\n",
    "print('Best alpha: ', best_alpha)\n",
    "\n",
    "plt.plot(res)\n",
    "plt.title('AUC vs. Alpha')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('AUC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695362e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "def evaluate_roc(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c2469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc_CV(model):\n",
    "    \"\"\"\n",
    "    Return the average AUC score from cross-validation.\n",
    "    \"\"\"\n",
    "    # Set KFold to shuffle data before the split\n",
    "    kf = StratifiedKFold(5, shuffle=True, random_state=1)\n",
    "\n",
    "    # Get AUC scores\n",
    "    auc = cross_val_score(\n",
    "        model, X_train_tfidf, y_train, scoring=\"roc_auc\", cv=kf)\n",
    "\n",
    "    return auc.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4b61ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.Series([get_auc_CV(MultinomialNB(i))\n",
    "                 for i in np.arange(1, 10, 0.1)],\n",
    "                index=np.arange(1, 10, 0.1))\n",
    "\n",
    "best_alpha = np.round(res.idxmax(), 2)\n",
    "print('Best alpha: ', best_alpha)\n",
    "\n",
    "plt.plot(res)\n",
    "plt.title('AUC vs. Alpha')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('AUC')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4025669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION TO CALCULATE F1 SCORE\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds,axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average = 'weighted')\n",
    "#FUNCTION FOR CALCULATING ACCURACY PER CLASS\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v:k for k,v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds,axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(label_dict_inverse[label])\n",
    "        print(\"accuracy \", len(y_preds[y_preds==label])/len(y_true))\n",
    "#FUNCTION FOR MODEL EVALUATION\n",
    "def evaluate(dataloader_val):\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300f8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader_train, \n",
    "                        desc ='Epoch {:1d}'.format(epoch),\n",
    "                        leave=False,\n",
    "                       disable=False\n",
    "                       )\n",
    "    for batch in progress_bar:\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = { 'input_ids' : batch[0],\n",
    "                 'attention_mask' : batch[1],\n",
    "                 'labels' : batch[2]\n",
    "                 }\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss':'{:.3f}'.format(loss.item()/len(batch))})\n",
    "    \n",
    "    # THIS SECTION OF CODE IS JUST FOR PRINTING VALUES AFTER EACH EPOCH.\n",
    "    torch.save(model.state_dict(), f'BERT_ft_epoch{epoch}.model')\n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)\n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_val)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 score (weighted): {val_f1}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a2f469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c64602a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17aeee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "d845ad351ef00b05e2eb94858c114d5926e344d05a067103918e64f083fd01f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
