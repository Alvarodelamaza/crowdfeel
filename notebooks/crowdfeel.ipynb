{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.utils import class_weight\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.special import softmax\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'Twitter_Data/train.json'\n",
    "val_path = 'Twitter_Data/validation.json'\n",
    "test_path = 'Twitter_Data/test.json'\n",
    "#cappy_path = 'Data/cappy.json'\n",
    "#bege_path = 'Data/berkcan.json'\n",
    "#netflix_path = 'Other Data/netflix.json'\n",
    "device = 'cuda' #set to cpu if you don't have gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(text):\n",
    "    final_text = ''\n",
    "    for word in text.split():\n",
    "        if word.startswith('@'):\n",
    "            continue\n",
    "        elif word[-3:] in ['com', 'org']:\n",
    "            continue\n",
    "        elif word.startswith('pic') or word.startswith('http') or word.startswith('www'):\n",
    "            continue\n",
    "        else:\n",
    "            final_text += word+' '\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "\n",
    "def feature_extraction(text):\n",
    "    x = tokenizer.encode(filter(text))\n",
    "    with torch.no_grad():\n",
    "        x, _ = bert(torch.stack([torch.tensor(x)]).to(device))\n",
    "        return list(x[0][0].cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'negative':0, 'positive':4}\n",
    "\n",
    "def data_prep(dataset):\n",
    "    X = []\n",
    "    y = []\n",
    "    for element in tqdm(dataset):\n",
    "        X.append(feature_extraction(element['sentence']))\n",
    "        y_val = np.zeros(2)\n",
    "        y_val[mapping[element['value']]] = 2\n",
    "        y.append(y_val)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "with open(train_path, 'r') as f:\n",
    "    train = json.load(f)\n",
    "with open(val_path, 'r') as f:\n",
    "    val = json.load(f)\n",
    "with open(test_path, 'r') as f:\n",
    "    test = json.load(f)\n",
    "\n",
    "\n",
    "X_train, y_train = data_prep(train)\n",
    "X_val, y_val = data_prep(val)\n",
    "X_test, y_test = data_prep(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced', np.unique(np.argmax(y_train, 1)), np.argmax(y_train, 1))\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='tanh', input_shape=(768,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', # categorical_crossentropy\n",
    "              optimizer=Adagrad(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(np.array(X_train), np.array(y_train),\n",
    "                    batch_size=64,\n",
    "                    epochs=500,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    class_weight=class_weights,\n",
    "                    callbacks = [es])\n",
    "\n",
    "y_true, y_pred = np.argmax(y_test, 1), np.argmax(model.predict(X_test), 1)\n",
    "print(classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('shims': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d42b486a2aeb5e987cabef571b82d7728a3739275d7b4b5605c7d88ce639d283"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
